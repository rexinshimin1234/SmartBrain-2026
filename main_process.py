# main_process.py
import os
from utils import split_text  # ğŸ‘ˆ çœ‹è¿™é‡Œï¼æˆ‘ä»¬è¦è°ƒç”¨ä½ åˆšæ‰å†™çš„å·¥å…·ç®±

def load_and_process(filename):
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(filename):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {filename}")
        return []

    # 2. è¯»å–æ–‡ä»¶ (The Reader)
    print(f"ğŸ“„ æ­£åœ¨è¯»å– {filename}...")
    with open(filename, 'r', encoding='utf-8') as f:
        content = f.read() 
    # 3. æ¸…æ´—æ•°æ® (å‡çº§ç‰ˆ)
    #    åŸç‰ˆ: clean_content = content.strip() 
    
    # --- æ–°ç‰ˆé€»è¾‘ ---
    # 1. å…ˆæŠŠæ¯ä¸€è¡Œåˆ‡å¼€
    lines = content.splitlines()
    # 2. åªæœ‰å½“è¿™ä¸€è¡Œä¸æ˜¯ç©ºçš„æ—¶å€™ï¼Œæ‰ä¿ç•™ (å»é™¤ç©ºè¡Œ)
    non_empty_lines = [line.strip() for line in lines if line.strip()]
    # 3. å†æŠŠå®ƒä»¬æ‹¼å›å»ï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥
    clean_content = "\n".join(non_empty_lines)
    
    print(f"ğŸ“Š æ¸…æ´—åå­—æ•°: {len(clean_content)}")
    
    print(f"ğŸ“Š åŸå§‹å­—æ•°: {len(clean_content)}")

    # 4. è°ƒç”¨æ‰‹æœ¯åˆ€è¿›è¡Œåˆ‡ç‰‡ (The Chunker)
    #    æˆ‘ä»¬è®¾å®šæ¯å— 100 å­—
    chunks = split_text(clean_content, chunk_size=300)
    
    print(f"ğŸ”ª åˆ‡åˆ†å®Œæˆï¼å…±åˆ‡æˆ {len(chunks)} å—ã€‚")
    return chunks

# --- ä¸»ç¨‹åº ---
if __name__ == "__main__":
    # å‡è®¾ä½ çš„æ•°æ®æ–‡ä»¶å« data.txt
    file_path = "data.txt"
    
    # è·‘æµç¨‹
    knowledge_base = load_and_process(file_path)

    # 5. æŠ½æŸ¥ä¸€ä¸‹ (çœ‹çœ‹åˆ‡å‡ºæ¥çš„ç¬¬ä¸€å—é•¿ä»€ä¹ˆæ ·)
    if knowledge_base:
        print("\n--- é¢„è§ˆç¬¬ä¸€å—æ•°æ® (Chunk 0) ---")
        print(knowledge_base[0])
        print("-------------------------------")
        
        print("\n--- é¢„è§ˆæœ€åä¸€å—æ•°æ® ---")
        print(knowledge_base[-1])
        print("-------------------------------")